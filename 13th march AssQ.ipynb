{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf7132a-7431-4c34-bde1-d25775d91e5a",
   "metadata": {},
   "source": [
    "Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\n",
    "the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d84fdf-5be7-4fb4-b602-73820ee35aa6",
   "metadata": {},
   "source": [
    "The assumptions for using ANOVA are:\n",
    "\n",
    "1. Independence: Observations within each group should be independent.\n",
    "\n",
    "2. Normality: The distribution of the dependent variable should be approximately normal within each group.\n",
    "\n",
    "3. Homogeneity of Variance: The variance of the dependent variable should be roughly equal across all groups.\n",
    "\n",
    "4. Homogeneity of Covariance: For multivariate ANOVA, the covariance between dependent variables should be equal across groups.\n",
    "\n",
    "Violations and their impacts:\n",
    "\n",
    "1. Violation of Independence: Non-independence of observations can introduce bias or confounding factors.\n",
    "\n",
    "2. Violation of Normality: Departure from normality can affect p-values and confidence intervals, leading to incorrect conclusions.\n",
    "\n",
    "3. Violation of Homogeneity of Variance: Unequal variances can result in inaccurate significance tests and confidence intervals.\n",
    "\n",
    "4. Violation of Homogeneity of Covariance: In multivariate ANOVA, differences in covariance between groups can affect the overall analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb900302-33aa-4ed7-86e8-2b5ec41c7252",
   "metadata": {},
   "source": [
    "Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f2924-9ce9-4269-80fc-7b033d9c2b01",
   "metadata": {},
   "source": [
    "The three types of ANOVA (Analysis of Variance) are:\n",
    "\n",
    "1. One-Way ANOVA:\n",
    "One-Way ANOVA is used when comparing the means of three or more groups or levels of a single independent variable. It determines whether there are any significant differences among the means of the groups. This type of ANOVA is appropriate when there is one categorical independent variable and a continuous dependent variable. One-Way ANOVA helps answer questions such as \"Is there a difference in test scores among students in different schools?\" or \"Does the mean income differ across different regions?\"\n",
    "\n",
    "2. Two-Way ANOVA:\n",
    "Two-Way ANOVA is used when studying the interaction effects between two independent variables on a dependent variable. It allows for examining the effects of each independent variable separately as well as their combined effect. This type of ANOVA is appropriate when there are two categorical independent variables and a continuous dependent variable. Two-Way ANOVA helps answer questions such as \"Does the effectiveness of a new drug depend on both gender and age?\" or \"Is there an interaction effect between different teaching methods and student backgrounds on test scores?\"\n",
    "\n",
    "3. Repeated Measures ANOVA:\n",
    "Repeated Measures ANOVA is used when analyzing the changes or differences in a dependent variable measured repeatedly over time or under different conditions. It is suitable for studying within-subject designs where the same individuals are measured multiple times. Repeated Measures ANOVA is commonly used in fields such as psychology and medicine to assess changes in response to interventions or treatments. It helps answer questions such as \"Does a therapy result in significant changes in anxiety levels over time?\" or \"Is there a difference in reaction times across different stimuli conditions?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b82de-8fda-4778-80e5-ff6ae95fc649",
   "metadata": {},
   "source": [
    "Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1eac6-367b-4de2-a4e8-7e004009a40b",
   "metadata": {},
   "source": [
    "The partitioning of variance in ANOVA refers to the decomposition of the total variance observed in a dataset into different sources of variation. It helps understand how much of the total variation in the data can be attributed to different factors or sources, such as group differences, experimental conditions, or error.\n",
    "\n",
    "In ANOVA, the total variance is partitioned into two components: the between-group variance and the within-group (or residual) variance. The between-group variance represents the variation between different groups or conditions being compared, while the within-group variance represents the variation within each group or condition.\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "1. Assessing Group Differences: By partitioning the variance, ANOVA enables us to determine whether the observed differences between groups or conditions are statistically significant. It helps answer questions such as \"Are there significant differences in mean scores between different treatment groups?\"\n",
    "\n",
    "2. Quantifying the Effects: ANOVA provides estimates of the amount of variance accounted for by different factors or sources, such as group membership or experimental conditions. This allows us to understand the relative importance of these factors in explaining the observed variation in the data.\n",
    "\n",
    "3. Identifying Error Variance: The within-group (residual) variance represents the random variation that cannot be attributed to the factors under study. It helps quantify the degree of variability that is not accounted for by the specific effects of interest. This is important for understanding the reliability and consistency of the observed effects.\n",
    "\n",
    "4. Hypothesis Testing: The partitioning of variance facilitates hypothesis testing by comparing the ratio of between-group variance to within-group variance (F-ratio). This ratio is used to determine whether the observed group differences are statistically significant, providing a basis for making inferences about the population.\n",
    "\n",
    "5. Experimental Design: Understanding the partitioning of variance helps in designing experiments and studies by considering the factors that contribute most to the variation. It aids in determining sample sizes, optimizing study designs, and selecting appropriate statistical models.\n",
    "\n",
    "In summary, the partitioning of variance in ANOVA allows for the quantification of different sources of variation, identification of significant group differences, and inference about population characteristics. It provides valuable insights into the factors driving variability in the data and helps in making informed statistical decisions and interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf9cdc-7d3f-4d9d-a389-88e066fed5fd",
   "metadata": {},
   "source": [
    "Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual\n",
    "sum of squares (SSR) in a one-way ANOVA using Python?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a842ce7-3e52-4252-9f95-fd75de8d26cf",
   "metadata": {},
   "source": [
    "To calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python, you can use the following steps:\n",
    "\n",
    "1. Calculate the overall mean (grand mean) of the data.\n",
    "\n",
    "2. Calculate the total sum of squares (SST) by subtracting each individual value from the grand mean, squaring the differences, and summing them up.\n",
    "\n",
    "3. Calculate the explained sum of squares (SSE) by calculating the sum of squares between groups. For each group, subtract the group mean from the grand mean, square the difference, and multiply it by the number of observations in that group. Sum up the squared differences for all groups.\n",
    "\n",
    "4. Calculate the residual sum of squares (SSR) by calculating the sum of squares within groups. For each observation, subtract the corresponding group mean, square the difference, and sum up the squared differences across all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcfd48d9-319b-4563-8111-c5985481278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_sst_sse_ssr(data, labels):\n",
    "    # Calculate the overall mean\n",
    "    grand_mean = np.mean(data)\n",
    "\n",
    "    # Initialize variables for SST, SSE, and SSR\n",
    "    sst = 0\n",
    "    sse = 0\n",
    "    ssr = 0\n",
    "\n",
    "    # Calculate SST, SSE, and SSR\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        group_data = data[labels == label]\n",
    "        group_mean = np.mean(group_data)\n",
    "\n",
    "        sst += np.sum((group_data - grand_mean) ** 2)\n",
    "        sse += np.sum((group_data - group_mean) ** 2)\n",
    "        ssr += np.sum((group_mean - grand_mean) ** 2) * len(group_data)\n",
    "\n",
    "    return sst, sse, ssr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18983d5e-6c98-4ca8-928e-99bb413e8ffe",
   "metadata": {},
   "source": [
    "Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953bb0b-c2e2-4ae3-b348-3be5d2411cd2",
   "metadata": {},
   "source": [
    "In a two-way ANOVA, the main effects represent the individual effects of each independent variable, while the interaction effect represents the combined effect of the two independent variables. To calculate the main effects and interaction effect using Python, you can follow these steps:\n",
    "\n",
    "1. Perform the Two-Way ANOVA: First, conduct the two-way ANOVA using any statistical package or library in Python, such as scipy.stats or statsmodels. This will provide you with the necessary analysis of variance table containing the sources of variation, degrees of freedom, sums of squares, and mean squares.\n",
    "\n",
    "2. Calculate the Main Effects: The main effects can be calculated by comparing the means of each independent variable across its levels or groups. You can calculate the group means using numpy or pandas, and then compare the means to assess the main effects.\n",
    "\n",
    "3. Calculate the Interaction Effect: The interaction effect represents the combined effect of the two independent variables. It can be calculated by examining the differences in means between the combination of levels or groups. You can calculate the group means for each combination and then compare the means to assess the interaction effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6562b487-3d6e-4d29-bfa3-6fa8661f64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_main_effects_interaction(anova_table, data):\n",
    "    # Get the group means for each factor combination\n",
    "    group_means = data.groupby(['factor1', 'factor2'])['response'].mean()\n",
    "\n",
    "    # Calculate the main effects\n",
    "    main_effect_factor1 = group_means.groupby('factor1').mean()\n",
    "    main_effect_factor2 = group_means.groupby('factor2').mean()\n",
    "\n",
    "    # Calculate the interaction effect\n",
    "    interaction_effect = group_means.unstack(level=0).diff(axis=1).mean()\n",
    "\n",
    "    return main_effect_factor1, main_effect_factor2, interaction_effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf1ae4-d25a-403f-8753-8bafc1c08d86",
   "metadata": {},
   "source": [
    "Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "What can you conclude about the differences between the groups, and how would you interpret these\n",
    "results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e7047-2cc2-4c56-bc26-8bfb5ecd58b4",
   "metadata": {},
   "source": [
    "Based on the obtained F-statistic of 5.23 and a p-value of 0.02 in a one-way ANOVA, we can draw the following conclusions:\n",
    "\n",
    "1. Differences between the groups: The obtained F-statistic indicates that there are statistically significant differences between the groups being compared. The groups exhibit variations in their means that are unlikely to have occurred by chance alone.\n",
    "\n",
    "2. Interpretation of the results: Since the p-value (0.02) is less than the significance level (usually set at 0.05), we reject the null hypothesis. The null hypothesis in this context states that there are no significant differences between the group means. Therefore, we can conclude that there are indeed significant differences between at least some of the groups.\n",
    "\n",
    "3. Post-hoc analyses: If the ANOVA indicates significant differences among the groups, it is common practice to conduct post-hoc analyses, such as Tukey's Honestly Significant Difference (HSD) test or Bonferroni correction, to determine which specific groups differ significantly from each other. These additional analyses provide further insights into the pairwise group comparisons.\n",
    "\n",
    "4. Effect size: In addition to the significance of the results, it is also important to consider the effect size, which quantifies the magnitude of the observed differences. The effect size can be calculated using measures such as eta-squared (η²) or Cohen's d. A larger effect size indicates a stronger practical significance of the observed differences.\n",
    "\n",
    "In summary, the obtained F-statistic of 5.23 and a p-value of 0.02 in a one-way ANOVA suggest that there are statistically significant differences between the groups being compared. This finding allows us to reject the null hypothesis and conclude that the group means are significantly different from each other. Post-hoc analyses and effect size calculations can provide further insights into the specific group differences and the practical significance of these findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363802f-701e-4955-b33a-2043ca68a577",
   "metadata": {},
   "source": [
    "Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\n",
    "consequences of using different methods to handle missing data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46521930-7576-47d4-9b52-fbe6a09c90f9",
   "metadata": {},
   "source": [
    "In a repeated measures ANOVA, handling missing data can be challenging but crucial for accurate analysis. Here are some common approaches and their potential consequences:\n",
    "\n",
    "Listwise deletion: This method involves removing any participant with missing data on any of the variables being analyzed. It provides a complete-case analysis by excluding participants with missing data. The potential consequences include reduced sample size, loss of statistical power, and potential bias if missing data are not missing completely at random.\n",
    "\n",
    "Pairwise deletion: This approach uses all available data for each pairwise comparison, even if some participants have missing data on certain variables. It maximizes the use of available data but can lead to biased results if missingness is not random. The consequences include potentially biased estimates of means and increased variability due to different sample sizes in each pairwise comparison.\n",
    "\n",
    "Imputation: Imputation methods estimate missing values based on observed data. Common imputation techniques include mean imputation, regression imputation, or multiple imputation. Imputation allows for the inclusion of all participants in the analysis and can reduce bias due to missing data. However, it assumes that the missing data mechanism is ignorable and that the imputation model accurately captures the relationship between variables. If these assumptions are violated, imputation can introduce additional uncertainty and potentially bias the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e83b0b-6200-452c-97ad-c51fb0801a80",
   "metadata": {},
   "source": [
    "Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c1e45-e08f-471a-b355-12130eef054f",
   "metadata": {},
   "source": [
    "After conducting an ANOVA and finding a significant overall effect, post-hoc tests are used to determine which specific group differences are significant. Some common post-hoc tests include:\n",
    "\n",
    "1. Tukey's Honestly Significant Difference (HSD): This test is widely used and compares all possible pairwise differences between group means. It controls the experiment-wise error rate and is appropriate when the sample sizes are equal. It is a conservative test, making it useful for situations where multiple pairwise comparisons need to be made.\n",
    "\n",
    "2. Bonferroni correction: This is a simple and conservative method that adjusts the significance level for multiple comparisons. It divides the desired alpha level by the number of comparisons to control the family-wise error rate. It is appropriate when conducting a large number of pairwise comparisons.\n",
    "\n",
    "3. Dunnett's test: This test compares the means of multiple treatment groups to a control group. It is useful when comparing several treatments to a single control group and is more powerful than conducting multiple t-tests against the control group.\n",
    "\n",
    "4. Scheffe's test: This test allows for comparisons of all possible combinations of groups while controlling the experiment-wise error rate. It is a flexible but conservative test that is appropriate when sample sizes are unequal and variances are not homogeneous.\n",
    "\n",
    "5. Games-Howell test: This test is a non-parametric alternative to Tukey's HSD and can be used when the assumptions of equal variances and normality are violated. It is appropriate when sample sizes and variances differ between groups.\n",
    "\n",
    "Example situation: Suppose a study compares the effectiveness of four different treatments for reducing anxiety levels. The ANOVA reveals a significant overall effect, indicating that at least one treatment differs from the others. In this case, a post-hoc test would be necessary to determine which specific treatment(s) differ significantly from each other. Tukey's HSD or Scheffe's test can be used to conduct pairwise comparisons and identify the significant differences in anxiety reduction between the treatment groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c1f0c-1874-47ec-bd5c-04735c9f09ca",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e0c48b-e593-4825-90f3-5186529a2510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 0.14899889846432965\n",
      "p-value: 0.8617000167643234\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Weight loss data for each diet\n",
    "diet_A = [2.3, 1.8, 3.1, 1.5, 2.9, 2.2, 1.7, 2.8, 3.0, 2.5,\n",
    "          1.9, 2.7, 2.1, 1.6, 2.4, 2.0, 2.6, 1.4, 2.8, 2.3,\n",
    "          1.7, 2.6, 2.2, 1.5, 2.9, 2.4, 1.8, 3.2, 2.7, 1.3,\n",
    "          2.5, 1.9, 2.8, 2.3, 1.6, 2.1, 1.7, 2.7, 3.0, 1.4,\n",
    "          2.6, 2.2, 1.5, 2.9, 2.4, 1.8, 3.2, 2.7, 1.3, 2.5]\n",
    "diet_B = [1.5, 2.0, 2.4, 1.9, 2.2, 1.7, 2.3, 2.5, 1.8, 2.6,\n",
    "          2.1, 1.6, 2.8, 2.0, 2.7, 2.2, 1.5, 2.9, 2.4, 1.8,\n",
    "          3.2, 2.7, 1.3, 2.5, 1.9, 2.8, 2.3, 1.6, 2.1, 1.7,\n",
    "          2.7, 3.0, 1.4, 2.6, 2.2, 1.5, 2.9, 2.4, 1.8, 3.2,\n",
    "          2.7, 1.3, 2.5, 1.9, 2.8, 2.3, 1.6, 2.1, 1.7, 2.7]\n",
    "diet_C = [2.1, 1.6, 2.4, 2.0, 2.7, 2.2, 1.5, 2.9, 2.4, 1.8,\n",
    "          3.2, 2.7, 1.3, 2.5, 1.9, 2.8, 2.3, 1.6, 2.1, 1.7,\n",
    "          2.7, 3.0, 1.4, 2.6, 2.2, 1.5, 2.9, 2.4, 1.8, 3.2,\n",
    "          2.7, 1.3, 2.5, 1.9, 2.8, 2.3, 1.6, 2.1, 1.7, 2.7,\n",
    "          3.0, 1.4, 2.6, 2.2, 1.5, 2.9, 2.4, 1.8, 3.2, 2.7]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Print the results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae00977-99b1-4d30-bbd4-c4c80dc40af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
